import os
import io
import asyncio
import logging
import traceback
import aiohttp
from aiogram import Bot, Dispatcher, Router, F, types
from aiogram.types import BufferedInputFile, FSInputFile
from aiogram.exceptions import TelegramRetryAfter
from aiogram.filters import Command

if not os.environ.get("BOT_TOKEN"):
    raise Exception('provide BOT_TOKEN in env')

logging.basicConfig(
    format='%(levelname)s: %(name)s[%(process)d] - %(asctime)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

BOT = Bot(os.environ.get("BOT_TOKEN"))
DP = Dispatcher()
router = Router()

# –ü–∞—Ä—Å–µ—Ä –º–∞–Ω–≥–∏
async def download_manga_chapter(url: str) -> tuple[str, bytes]:
    """–°–∫–∞—á–∏–≤–∞–µ—Ç –≥–ª–∞–≤—É –º–∞–Ω–≥–∏ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç (–∏–º—è_—Ñ–∞–π–ª–∞, zip_–¥–∞–Ω–Ω—ã–µ)"""
    import zipfile
    from bs4 import BeautifulSoup
    import re
    
    async with aiohttp.ClientSession() as session:
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            async with session.get(url, headers=headers) as response:
                if response.status != 200:
                    raise Exception(f"HTTP {response.status}")
                
                html = await response.text()
                soup = BeautifulSoup(html, 'lxml')
                
                # –ò—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –º–∞–Ω–≥–∏
                images = []
                
                # –ü–æ–ø—ã—Ç–∫–∞ –Ω–∞–π—Ç–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö
                # –í–∞—Ä–∏–∞–Ω—Ç 1: –ø—Ä—è–º—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ img —Ç–µ–≥–∞—Ö
                img_tags = soup.find_all('img')
                for img in img_tags:
                    src = img.get('src') or img.get('data-src') or img.get('data-lazy-src')
                    if src:
                        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ URL –≤ –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ
                        if src.startswith('//'):
                            src = 'https:' + src
                        elif src.startswith('/'):
                            from urllib.parse import urljoin
                            src = urljoin(url, src)
                        elif not src.startswith('http'):
                            from urllib.parse import urljoin
                            src = urljoin(url, src)
                        
                        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –º–∞–Ω–≥–∏ (–æ–±—ã—á–Ω–æ jpg, png, webp)
                        if any(ext in src.lower() for ext in ['.jpg', '.jpeg', '.png', '.webp', '.gif']):
                            if 'manga' in src.lower() or 'chapter' in src.lower() or 'page' in src.lower() or 'img' in src.lower():
                                images.append(src)
                
                # –í–∞—Ä–∏–∞–Ω—Ç 2: –ø–æ–∏—Å–∫ –≤ JavaScript –¥–∞–Ω–Ω—ã—Ö
                scripts = soup.find_all('script')
                for script in scripts:
                    if script.string:
                        # –ò—â–µ–º –º–∞—Å—Å–∏–≤—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ JS
                        matches = re.findall(r'["\']([^"\']*\.(?:jpg|jpeg|png|webp|gif)[^"\']*)["\']', script.string, re.IGNORECASE)
                        for match in matches:
                            if any(keyword in match.lower() for keyword in ['manga', 'chapter', 'page', 'img']):
                                if match.startswith('//'):
                                    match = 'https:' + match
                                elif match.startswith('/'):
                                    from urllib.parse import urljoin
                                    match = urljoin(url, match)
                                elif not match.startswith('http'):
                                    from urllib.parse import urljoin
                                    match = urljoin(url, match)
                                if match not in images:
                                    images.append(match)
                
                # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –ø—Ä–æ–±—É–µ–º —Å–∫–∞—á–∞—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É –∫–∞–∫ –µ—Å—Ç—å
                if not images:
                    # –°–æ–∑–¥–∞–µ–º ZIP —Å HTML —Å—Ç—Ä–∞–Ω–∏—Ü–µ–π
                    zip_buffer = io.BytesIO()
                    with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
                        zip_file.writestr("page.html", html.encode('utf-8'))
                        zip_file.writestr("readme.txt", f"Manga page downloaded from: {url}\n\nThis is a basic downloader. For full functionality, add a specific parser for your manga source.")
                    zip_buffer.seek(0)
                    filename = "manga_page.zip"
                    return filename, zip_buffer.read()
                
                # –°–∫–∞—á–∏–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                zip_buffer = io.BytesIO()
                downloaded_count = 0
                
                with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
                    for i, img_url in enumerate(images[:50], 1):  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 50 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
                        try:
                            async with session.get(img_url, headers=headers, timeout=aiohttp.ClientTimeout(total=30)) as img_response:
                                if img_response.status == 200:
                                    img_data = await img_response.read()
                                    if len(img_data) > 1000:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ —Ä–µ–∞–ª—å–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                                        ext = '.jpg'
                                        if '.png' in img_url.lower():
                                            ext = '.png'
                                        elif '.webp' in img_url.lower():
                                            ext = '.webp'
                                        elif '.gif' in img_url.lower():
                                            ext = '.gif'
                                        
                                        zip_file.writestr(f"{i:03d}{ext}", img_data)
                                        downloaded_count += 1
                        except Exception as e:
                            logger.warning(f"Failed to download image {img_url}: {e}")
                            continue
                
                if downloaded_count == 0:
                    raise Exception("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –º–∞–Ω–≥–∏")
                
                zip_buffer.seek(0)
                filename = f"manga_chapter_{downloaded_count}_pages.zip"
                return filename, zip_buffer.read()
                
        except Exception as e:
            logger.error(f"Error downloading manga: {e}")
            raise

@router.message(Command("start"))
async def cmd_start(message: types.Message):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /start"""
    await message.answer(
        "–ü—Ä–∏–≤–µ—Ç! –Ø –±–æ—Ç –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –º–∞–Ω–≥–∏.\n\n"
        "–û—Ç–ø—Ä–∞–≤—å –º–Ω–µ URL –º–∞–Ω–≥–∏ –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π –∫–æ–º–∞–Ω–¥—É /help –¥–ª—è —Å–ø—Ä–∞–≤–∫–∏."
    )

@router.message(Command("help"))
async def cmd_help(message: types.Message):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /help"""
    await message.answer(
        "üìö <b>–ë–æ—Ç –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –º–∞–Ω–≥–∏</b>\n\n"
        "–û—Ç–ø—Ä–∞–≤—å –º–Ω–µ URL –º–∞–Ω–≥–∏, –∏ —è —Å–∫–∞—á–∞—é –µ—ë –¥–ª—è —Ç–µ–±—è.\n\n"
        "–ü—Ä–∏–º–µ—Ä—ã:\n"
        "‚Ä¢ –û—Ç–ø—Ä–∞–≤—å —Å—Å—ã–ª–∫—É –Ω–∞ –≥–ª–∞–≤—É –º–∞–Ω–≥–∏\n"
        "‚Ä¢ –ë–æ—Ç —Å–∫–∞—á–∞–µ—Ç –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ –æ—Ç–ø—Ä–∞–≤–∏—Ç ZIP –∞—Ä—Ö–∏–≤\n\n"
        "–ö–æ–º–∞–Ω–¥—ã:\n"
        "/start - –ù–∞—á–∞—Ç—å —Ä–∞–±–æ—Ç—É\n"
        "/help - –ü–æ–∫–∞–∑–∞—Ç—å —ç—Ç—É —Å–ø—Ä–∞–≤–∫—É",
        parse_mode="HTML"
    )

@router.message(F.text & F.text.startswith("http"))
async def handle_url(message: types.Message):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ URL –º–∞–Ω–≥–∏"""
    url = message.text.strip()
    
    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –æ –Ω–∞—á–∞–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏
    status_msg = await message.answer("‚è≥ –ù–∞—á–∏–Ω–∞—é —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –º–∞–Ω–≥–∏...")
    
    try:
        filename, zip_data = await download_manga_chapter(url)
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º ZIP —Ñ–∞–π–ª
        document = BufferedInputFile(zip_data, filename)
        await BOT.send_document(
            chat_id=message.chat.id,
            document=document,
            caption=f"üì¶ –ú–∞–Ω–≥–∞ —Å–∫–∞—á–∞–Ω–∞\nURL: {url}",
            request_timeout=600
        )
        
        # –£–¥–∞–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –æ —Å—Ç–∞—Ç—É—Å–µ
        await status_msg.delete()
        
    except TelegramRetryAfter as e:
        await asyncio.sleep(e.retry_after)
        await handle_url(message)
    except Exception as e:
        logger.error(f"Error processing manga URL: {e}")
        traceback.print_exc()
        await status_msg.edit_text(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ –º–∞–Ω–≥–∏: {str(e)}")

@router.message()
async def handle_other(message: types.Message):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –ø—Ä–æ—á–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π"""
    await message.answer(
        "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –æ—Ç–ø—Ä–∞–≤—å –º–Ω–µ URL –º–∞–Ω–≥–∏ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è.\n"
        "–ò—Å–ø–æ–ª—å–∑—É–π /help –¥–ª—è —Å–ø—Ä–∞–≤–∫–∏."
    )

async def main() -> None:
    DP.include_router(router)
    await DP.start_polling(BOT, polling_timeout=5, handle_signals=False)

if __name__ == "__main__":
    asyncio.run(main())
